import pandas as pd
import numpy as np
import argparse
import os
import sys

def create_sample_data(input_file=None, output_file="UserBehavior_sampled.csv", sample_ratio=0.01):
    """
    ç”¨æˆ·è¡Œä¸ºæ•°æ®é¢„å¤„ç†å·¥å…·
    """
    
    if input_file and os.path.exists(input_file):
        print(f"ğŸ” æ­£åœ¨ä» {input_file} æŠ½æ · {sample_ratio*100}% æ•°æ®...")
        
        try:
            # ç›´æ¥è¯»å–æŠ½æ ·
            df = pd.read_csv(
                input_file,
                names=["user_id", "item_id", "category_id", "behavior_type", "timestamp"],
                header=None,
                low_memory=False
            )
            
            print(f"ğŸ“Š åŸå§‹æ•°æ®è¡Œæ•°: {len(df):,}")
            
            # æŠ½æ ·
            sampled_df = df.sample(frac=sample_ratio, random_state=42)
            sampled_df.to_csv(output_file, index=False, header=False)
            
            print(f"âœ… æŠ½æ ·å®Œæˆï¼")
            print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {output_file}")
            print(f"ğŸ“ˆ æŠ½æ ·æ•°æ®: {len(sampled_df):,} è¡Œ")
            
        except MemoryError:
            # åˆ†å—è¯»å–
            print("âš ï¸  å†…å­˜ä¸è¶³ï¼Œä½¿ç”¨åˆ†å—è¯»å–...")
            process_large_file_in_chunks(input_file, output_file, sample_ratio)
            
    else:
        # ç”Ÿæˆç¤ºä¾‹æ•°æ®
        print("ğŸ² æ­£åœ¨ç”Ÿæˆç¤ºä¾‹æ•°æ®...")
        generate_demo_data(output_file)

def process_large_file_in_chunks(input_file, output_file, sample_ratio, chunksize=100000):
    """åˆ†å—å¤„ç†å¤§æ–‡ä»¶"""
    chunks = []
    total_rows = 0
    
    for chunk_num, chunk in enumerate(pd.read_csv(
        input_file,
        names=["user_id", "item_id", "category_id", "behavior_type", "timestamp"],
        header=None,
        chunksize=chunksize,
        low_memory=False
    )):
        sampled_chunk = chunk.sample(frac=sample_ratio, random_state=42)
        chunks.append(sampled_chunk)
        total_rows += len(chunk)
        
        if chunk_num % 10 == 0:
            print(f"ğŸ“¦ å·²å¤„ç†: {total_rows:,} è¡Œ")
    
    result = pd.concat(chunks, ignore_index=True)
    result.to_csv(output_file, index=False, header=False)
    
    print(f"âœ… å¤„ç†å®Œæˆï¼")
    print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {output_file}")
    print(f"ğŸ“ˆ æ€»è¡Œæ•°: {len(result):,}")

def generate_demo_data(output_file, n_samples=50000):
    """ç”Ÿæˆæ¼”ç¤ºæ•°æ®"""
    np.random.seed(42)
    
    demo_data = pd.DataFrame({
        'user_id': np.random.randint(1, 1000, n_samples),
        'item_id': np.random.randint(1, 5000, n_samples),
        'category_id': np.random.randint(1, 100, n_samples),
        'behavior_type': np.random.choice(['pv', 'fav', 'cart', 'buy'], n_samples, p=[0.7, 0.1, 0.15, 0.05]),
        'timestamp': np.random.randint(1577808000, 1577894400, n_samples)
    })
    
    demo_data.to_csv(output_file, index=False, header=False)
    print(f"âœ… ç¤ºä¾‹æ•°æ®å·²ç”Ÿæˆ: {output_file}")
    print(f"ğŸ“Š æ•°æ®è¡Œæ•°: {len(demo_data):,}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='ç”¨æˆ·è¡Œä¸ºæ•°æ®é¢„å¤„ç†å·¥å…·')
    parser.add_argument('--input', help='è¾“å…¥æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰', default=None)
    parser.add_argument('--output', default='UserBehavior_sampled.csv', help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--ratio', type=float, default=0.01, help='æŠ½æ ·æ¯”ä¾‹ (0.01 = 1%%)')
    
    args = parser.parse_args()
    
    print("=" * 50)
    print("ğŸ›’ ç”µå•†ç”¨æˆ·è¡Œä¸ºæ•°æ®é¢„å¤„ç†å·¥å…·")
    print("=" * 50)
    
    create_sample_data(args.input, args.output, args.ratio)